{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Deterministic Behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import data_operations as data_ops\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D,  MaxPooling1D, BatchNormalization, GlobalMaxPooling1D\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Set the seed value for experiment reproducibility.\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data/SAVEE\"\n",
    "\n",
    "TRAIN_SIZE, VAL_SIZE, TEST_SIZE = (300, 100, 80)\n",
    "\n",
    "SAMPLE_RATE_HZ = 44100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load audio and Get Label functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "  parts = re.sub('.+\\_|[0-9]+.wav', '', file_path)\n",
    "  return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [f'{DATA_DIR}/{p}' for p in os.listdir(DATA_DIR)]\n",
    "np.random.shuffle(filenames)\n",
    "labels = pd.get_dummies(list(map(get_label, filenames))).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size 300\n",
      "Validation set size 100\n",
      "Test set size 80\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./data/SAVEE/DC_n14.wav', array([0, 0, 0, 0, 1, 0, 0], dtype=uint8))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files, train_labels = (filenames[:TRAIN_SIZE], labels[:TRAIN_SIZE])\n",
    "val_files, val_labels = (filenames[TRAIN_SIZE: TRAIN_SIZE + VAL_SIZE], labels[TRAIN_SIZE: TRAIN_SIZE + VAL_SIZE])\n",
    "test_files, test_labels = (filenames[-TEST_SIZE:], labels[-TEST_SIZE:])\n",
    "\n",
    "print('Training set size', len(train_files))\n",
    "print('Validation set size', len(val_files))\n",
    "print('Test set size', len(test_files))\n",
    "(train_files[0], train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [\n",
    "    data_ops.ReadFile(),\n",
    "    data_ops.DecodeWav(),\n",
    "    data_ops.Squeeze(),\n",
    "    data_ops.Crop(SAMPLE_RATE_HZ * 8),\n",
    "    data_ops.ZeroPad(SAMPLE_RATE_HZ * 8),\n",
    "    data_ops.CastToFloat(),\n",
    "    data_ops.Reshape((SAMPLE_RATE_HZ * 8, 1))\n",
    "]\n",
    "\n",
    "train_ds = tfio.audio.AudioIODataset.from_tensor_slices((train_files, train_labels))\n",
    "val_ds = tfio.audio.AudioIODataset.from_tensor_slices((val_files, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1275.],\n",
       "        [1279.],\n",
       "        [1282.],\n",
       "        ...,\n",
       "        [   0.],\n",
       "        [   0.],\n",
       "        [   0.]], dtype=float32),\n",
       " array([0, 0, 0, 0, 1, 0, 0], dtype=uint8))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for o in operations:\n",
    "    train_ds = train_ds.map(o, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "first_el = list(train_ds.as_numpy_iterator())[0]\n",
    "first_el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1297.],\n",
       "        [1293.],\n",
       "        [1290.],\n",
       "        ...,\n",
       "        [   0.],\n",
       "        [   0.],\n",
       "        [   0.]], dtype=float32),\n",
       " array([0, 0, 0, 0, 0, 1, 0], dtype=uint8))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# val_ds = tfio.audio.AudioIODataset.from_tensor_slices(val_files)\n",
    "for o in operations:\n",
    "    val_ds = val_ds.map(o, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "first_el = list(val_ds.as_numpy_iterator())[0]\n",
    "first_el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows = 3\n",
    "# cols = 3\n",
    "# n = rows * cols\n",
    "# fig, axes = plt.subplots(rows, cols, figsize=(12, 15))\n",
    "\n",
    "# for i, (audio, label) in enumerate(train_ds.take(n)):\n",
    "#   r = i // cols\n",
    "#   c = i % cols\n",
    "#   ax = axes[r][c]\n",
    "#   ax.plot(audio.numpy())\n",
    "#   print(audio.numpy().shape)\n",
    "#   label = label.numpy().decode('utf-8')\n",
    "#   ax.set_title(label)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architecture\n",
    " * 6x 1D + Batch norm + max pooling\n",
    " * 1x 1D + Batch norm + global max pooling\n",
    " * 2x dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_filter_size =  (32, 21)\n",
    "filters = [64, 128, 256, 512, 1024]\n",
    "sizes = [19, 17, 15, 13, 11]\n",
    "middle_filters_size = list(zip(filters, sizes))\n",
    "\n",
    "activation= 'relu'\n",
    "pool_size = 2\n",
    "\n",
    "last_filter_size =  (1024, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352800"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_RATE_HZ * 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 352780, 32)        704       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 352780, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 176390, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 176372, 64)        38976     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 176372, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 88186, 64)        0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 88170, 128)        139392    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 88170, 128)       512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 44085, 128)       0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 44071, 256)        491776    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 44071, 256)       1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 22035, 256)       0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 22023, 512)        1704448   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 22023, 512)       2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 11011, 512)       0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 11001, 1024)       5768192   \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 11001, 1024)      4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_5 (MaxPooling  (None, 5500, 1024)       0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_6 (Conv1D)           (None, 5492, 1024)        9438208   \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 5492, 1024)       4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 1024)             0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               131200    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 903       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,725,959\n",
      "Trainable params: 17,719,879\n",
      "Non-trainable params: 6,080\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# first layer\n",
    "# input shape (None, n) = variable-length sequences of n-dimensional vectors\n",
    "model.add(Conv1D(first_filter_size[0], first_filter_size[1], activation = activation, input_shape=(SAMPLE_RATE_HZ * 8, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size = pool_size))\n",
    "\n",
    "# middle layers\n",
    "for (filter_size, kernel_size) in middle_filters_size:\n",
    "    model.add(Conv1D(filter_size, kernel_size, activation = activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size = pool_size))\n",
    "\n",
    "# last layer\n",
    "model.add(Conv1D(last_filter_size[0], last_filter_size[1], activation = activation))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(128, activation=activation))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.batch(32)\n",
    "val_ds = val_ds.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(None, 352800, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 7), dtype=tf.uint8, name=None))>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ScopedTFGraph.__del__ at 0x00000184B9EDE700>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Programmi\\venvs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\c_api_util.py\", line 54, in __del__\n",
      "    self.deleter(self.graph)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# model.fit(train_files['padded'].map(lambda x : tf.convert_to_tensor(x.numpy())), train_files['label'], epochs=10, validation_data=(val_files['padded'].map(lambda x : tf.convert_to_tensor(x.numpy())), val_files['label']))\n",
    "model.fit(train_ds, epochs=10, validation_data=val_ds)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e5983e64bdf840368ab29cea7e1e8f9b41dcdac8639741a33a347d09d965169"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
